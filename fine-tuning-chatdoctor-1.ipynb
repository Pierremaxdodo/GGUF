{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I. Finetuning PierreMaxime/llama-3-8b-chat-doctor","metadata":{}},{"cell_type":"markdown","source":"### 1. Import des modules et connexion HF et Wandb","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-17T15:37:04.925244Z","iopub.execute_input":"2024-06-17T15:37:04.925683Z","iopub.status.idle":"2024-06-17T15:38:56.578178Z","shell.execute_reply.started":"2024-06-17T15:37:04.925639Z","shell.execute_reply":"2024-06-17T15:38:56.576836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import des différents modules et bibliothèques \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:46:25.054045Z","iopub.execute_input":"2024-06-12T13:46:25.054366Z","iopub.status.idle":"2024-06-12T13:46:43.668405Z","shell.execute_reply.started":"2024-06-12T13:46:25.054336Z","shell.execute_reply":"2024-06-12T13:46:43.667432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Connexion à Hugging Face et à Wandb\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n\n!git config --global credential.helper store\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF\")\n\nlogin(token=hf_token, add_to_git_credential=True)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\n\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:46:43.669861Z","iopub.execute_input":"2024-06-12T13:46:43.670972Z","iopub.status.idle":"2024-06-12T13:47:04.522434Z","shell.execute_reply.started":"2024-06-12T13:46:43.670934Z","shell.execute_reply":"2024-06-12T13:47:04.521191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Importation du dataset","metadata":{}},{"cell_type":"code","source":"# Introduire le modèle de base + Dataset de training + nom du nouveau modèle\nbase_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ndataset_name = \"ruslanmv/ai-medical-chatbot\"\nnew_model = \"llama-3-8b-chat-doctor\"","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:47:04.525326Z","iopub.execute_input":"2024-06-12T13:47:04.525728Z","iopub.status.idle":"2024-06-12T13:47:04.531711Z","shell.execute_reply.started":"2024-06-12T13:47:04.525679Z","shell.execute_reply":"2024-06-12T13:47:04.530521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importation du dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(2000))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Preprocessing ","metadata":{}},{"cell_type":"code","source":"# Définition du format des données (tout mettre dans une colonne texte)\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\n# Application de la fonction de formatage \ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\nlen(dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Définir le nombre de données qui seront utilisées pour le test \ndataset = dataset.train_test_split(test_size=0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Modification de Llama3","metadata":{}},{"cell_type":"code","source":"torch_dtype = torch.float16 # Définition du type de données utilisé par PyTorch pour les calculs tensoriels\nattn_implementation = \"eager\" # Opération excécutée immédiatement (top pour le jupyter notebook) ≠ \"graph\" (mieux pour des gros modèles)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:47:04.532938Z","iopub.execute_input":"2024-06-12T13:47:04.533231Z","iopub.status.idle":"2024-06-12T13:47:04.540638Z","shell.execute_reply.started":"2024-06-12T13:47:04.533200Z","shell.execute_reply":"2024-06-12T13:47:04.539717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration de QLoRA (méthode de quantification : Quantized Low Rank Adapter), obligatoire car on a une contrainte de mémoire\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Chargement du modèle\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:47:04.541645Z","iopub.execute_input":"2024-06-12T13:47:04.541900Z","iopub.status.idle":"2024-06-12T13:48:55.868964Z","shell.execute_reply.started":"2024-06-12T13:47:04.541877Z","shell.execute_reply":"2024-06-12T13:48:55.867895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chargement du tokenizer (ChatML template qui distingue l'user de l'assistant)\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:48:55.870344Z","iopub.execute_input":"2024-06-12T13:48:55.870767Z","iopub.status.idle":"2024-06-12T13:48:56.477889Z","shell.execute_reply.started":"2024-06-12T13:48:55.870725Z","shell.execute_reply":"2024-06-12T13:48:56.476611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Training","metadata":{}},{"cell_type":"code","source":"# Configuration de LoRA, paramètres pour améliorer le temps d'entrainement \npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:48:56.479304Z","iopub.execute_input":"2024-06-12T13:48:56.479619Z","iopub.status.idle":"2024-06-12T13:48:57.335788Z","shell.execute_reply.started":"2024-06-12T13:48:56.479593Z","shell.execute_reply":"2024-06-12T13:48:57.334623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Définir les arguments de training \ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=4,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:49:03.236987Z","iopub.execute_input":"2024-06-12T13:49:03.237306Z","iopub.status.idle":"2024-06-12T13:49:03.269834Z","shell.execute_reply.started":"2024-06-12T13:49:03.237281Z","shell.execute_reply":"2024-06-12T13:49:03.268730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration du processus d'entraînement \ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T13:49:03.271171Z","iopub.execute_input":"2024-06-12T13:49:03.271566Z","iopub.status.idle":"2024-06-12T13:49:05.274834Z","shell.execute_reply.started":"2024-06-12T13:49:03.271527Z","shell.execute_reply":"2024-06-12T13:49:05.273852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-06-12T17:08:08.124758Z","iopub.execute_input":"2024-06-12T17:08:08.125058Z","iopub.status.idle":"2024-06-12T17:08:08.129518Z","shell.execute_reply.started":"2024-06-12T17:08:08.125033Z","shell.execute_reply":"2024-06-12T17:08:08.128613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Inférence sur le modèle non merge avec llama3","metadata":{}},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have bad headache. How do I get rid of it?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-06-12T17:08:12.247809Z","iopub.execute_input":"2024-06-12T17:08:12.248151Z","iopub.status.idle":"2024-06-12T17:08:29.697776Z","shell.execute_reply.started":"2024-06-12T17:08:12.248125Z","shell.execute_reply":"2024-06-12T17:08:29.696821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Enregistrement du modèle finetune","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T17:16:58.463279Z","iopub.execute_input":"2024-06-12T17:16:58.464236Z","iopub.status.idle":"2024-06-12T17:17:00.703767Z","shell.execute_reply.started":"2024-06-12T17:16:58.464195Z","shell.execute_reply":"2024-06-12T17:17:00.702700Z"},"trusted":true},"execution_count":null,"outputs":[]}]}