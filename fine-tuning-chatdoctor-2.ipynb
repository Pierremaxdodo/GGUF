{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":182527235,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28083}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# II. Merge 8b-chat-doctor avec Llama3 ","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl","metadata":{"execution":{"iopub.status.busy":"2024-06-14T07:59:31.265456Z","iopub.execute_input":"2024-06-14T07:59:31.266198Z","iopub.status.idle":"2024-06-14T08:00:35.021417Z","shell.execute_reply.started":"2024-06-14T07:59:31.266160Z","shell.execute_reply":"2024-06-14T08:00:35.020172Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF \")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T13:07:53.122227Z","iopub.execute_input":"2024-06-14T13:07:53.122826Z","iopub.status.idle":"2024-06-14T13:07:53.870558Z","shell.execute_reply.started":"2024-06-14T13:07:53.122800Z","shell.execute_reply":"2024-06-14T13:07:53.869578Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importation des bibliothèques nécessaires\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom peft import PeftModel\nimport torch\nfrom trl import setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-06-14T12:51:36.407606Z","iopub.execute_input":"2024-06-14T12:51:36.408350Z","iopub.status.idle":"2024-06-14T12:51:51.456561Z","shell.execute_reply.started":"2024-06-14T12:51:36.408318Z","shell.execute_reply":"2024-06-14T12:51:51.455214Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-06-14 12:51:42.338554: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-14 12:51:42.338663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-14 12:51:42.469086: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importation des bibliothèques nécessaires\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_chat_format\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"],"ename":"ModuleNotFoundError","evalue":"No module named 'peft'","output_type":"error"}]},{"cell_type":"code","source":"# Définir les chemins des modèles\nbase_model_path = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\nnew_model_path = \"PierreMaxime/llama-3-8b-chat-doctor\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_path)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_path,\n    return_dict=True,\n    low_cpu_mem_usage=True, # Pour l'optimisation de la mémoire CPU\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nbase_model, tokenizer = setup_chat_format(base_model, tokenizer)\n\n# Charger l'adaptateur et le fusionner avec le modèle de base\npeft_model = PeftModel.from_pretrained(base_model, new_model_path)\nmerged_model = peft_model.merge_and_unload()\n\n# Sauvegarder le modèle fusionné pour une utilisation ultérieure\noutput_model_path = \"/kaggle/working/llama-3-8b-chat-doctor-merged\"\nmerged_model.save_pretrained(output_model_path, safe_serialization=False)\ntokenizer.save_pretrained(output_model_path)\n\n\nprint(\"Le modèle fusionné a été sauvegardé à l'emplacement :\", output_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T08:03:02.499105Z","iopub.execute_input":"2024-06-14T08:03:02.500092Z","iopub.status.idle":"2024-06-14T08:05:34.158101Z","shell.execute_reply.started":"2024-06-14T08:03:02.500057Z","shell.execute_reply":"2024-06-14T08:05:34.157011Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91963b89fd142379857ffca15032f9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/750 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43093a21bb6d4643a73227cacfa188b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"751bcfb30b15434bb7ae5f22271ca8a0"}},"metadata":{}},{"name":"stdout","text":"Le modèle fusionné a été sauvegardé à l'emplacement : /kaggle/working/llama-3-8b-chat-doctor-merged\n","output_type":"stream"}]},{"cell_type":"code","source":"# Pousser le modèle sur Hugging Face \nfrom huggingface_hub import login, upload_folder, create_repo\n\nrepo_id = \"PierreMaxime/llama-3-8b-chat-doctor-merged\"\ncreate_repo(repo_id, exist_ok=True, private=False) ","metadata":{"execution":{"iopub.status.busy":"2024-06-14T08:12:59.615778Z","iopub.execute_input":"2024-06-14T08:12:59.616469Z","iopub.status.idle":"2024-06-14T08:12:59.949366Z","shell.execute_reply.started":"2024-06-14T08:12:59.616433Z","shell.execute_reply":"2024-06-14T08:12:59.948387Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/PierreMaxime/llama-3-8b-chat-doctor-merged', endpoint='https://huggingface.co', repo_type='model', repo_id='PierreMaxime/llama-3-8b-chat-doctor-merged')"},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import login, upload_folder\n\nupload_folder(\n    repo_id=repo_id,\n    folder_path=output_model_path,\n    commit_message=\"Ajout du modèle fusionné\",\n    use_auth_token=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T08:13:27.638552Z","iopub.execute_input":"2024-06-14T08:13:27.639447Z","iopub.status.idle":"2024-06-14T08:16:22.015326Z","shell.execute_reply.started":"2024-06-14T08:13:27.639413Z","shell.execute_reply":"2024-06-14T08:16:22.014345Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00004.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54bf52e1e8143379ebbcc20b9a139ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00004.bin:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92f3d34151ca4a8ab0dedb73717fe6b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00004.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"395ecb0228e7425a96447d9582cfa2df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00004.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"982f624e668b48719481ba445666dbc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beba4b8a141c499590855cf53982be05"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/PierreMaxime/llama-3-8b-chat-doctor-merged/commit/99346f862c60c91461034e73251500ed43182e5a', commit_message='Ajout du modèle fusionné', commit_description='', oid='99346f862c60c91461034e73251500ed43182e5a', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"output_model_path","metadata":{"execution":{"iopub.status.busy":"2024-06-14T08:16:35.076229Z","iopub.execute_input":"2024-06-14T08:16:35.076608Z","iopub.status.idle":"2024-06-14T08:16:35.083042Z","shell.execute_reply.started":"2024-06-14T08:16:35.076574Z","shell.execute_reply":"2024-06-14T08:16:35.081928Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/llama-3-8b-chat-doctor-merged'"},"metadata":{}}]}]}