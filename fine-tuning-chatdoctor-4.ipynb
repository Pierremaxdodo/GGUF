{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66280,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":55284}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IV. Convertir le modèle au format GGUF","metadata":{}},{"cell_type":"markdown","source":"Au préalable, enregistrer le modèle réalisé lors de l'étape III. dans Kaggle puis intégrer le modèle à ce notebook en cliquant sur \"+ add input\"","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF \")\nlogin(token = hf_token)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:22:12.107300Z","iopub.execute_input":"2024-06-17T13:22:12.107747Z","iopub.status.idle":"2024-06-17T13:22:12.416337Z","shell.execute_reply.started":"2024-06-17T13:22:12.107698Z","shell.execute_reply":"2024-06-17T13:22:12.415410Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# Clone d'un code qui convertit des fichiers au format GGUF\n%cd /kaggle/working\n!git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n%cd /kaggle/working/llama.cpp\n!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n!LLAMA_CUDA=1 conda run -n base make -j > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-17T13:22:15.850312Z","iopub.execute_input":"2024-06-17T13:22:15.850687Z","iopub.status.idle":"2024-06-17T13:22:50.326244Z","shell.execute_reply.started":"2024-06-17T13:22:15.850652Z","shell.execute_reply":"2024-06-17T13:22:50.324970Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'llama.cpp' already exists and is not an empty directory.\n/kaggle/working/llama.cpp\n","output_type":"stream"}]},{"cell_type":"code","source":"!python convert-hf-to-gguf.py /kaggle/input/doctor-merged/transformers/version1/1 \\\n    --outfile /kaggle/working/llama-3-8b-chat-doctor-merged.gguf \\\n    --outtype f16","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:23:02.682862Z","iopub.execute_input":"2024-06-17T13:23:02.683215Z","iopub.status.idle":"2024-06-17T13:26:32.066096Z","shell.execute_reply.started":"2024-06-17T13:23:02.683189Z","shell.execute_reply":"2024-06-17T13:26:32.065141Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nWriting: 100%|██████████████████████████| 16.1G/16.1G [03:14<00:00, 82.4Mbyte/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:35:45.725717Z","iopub.execute_input":"2024-06-17T13:35:45.726594Z","iopub.status.idle":"2024-06-17T13:35:46.669547Z","shell.execute_reply.started":"2024-06-17T13:35:45.726561Z","shell.execute_reply":"2024-06-17T13:35:46.668661Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"llama-3-8b-chat-doctor-merged.gguf  llama.cpp\n","output_type":"stream"}]},{"cell_type":"code","source":"# Pousser le modèle sur Hugging Face \n\n!pip install huggingface_hub\n\nfrom huggingface_hub import HfApi, HfFolder\nfrom huggingface_hub import create_repo, upload_file\n\nrepo_id = \"PierreMaxime/llama-3-8b-chat-doctor-merged.gguf\"\n\napi = HfApi()\n\napi.create_repo(repo_id=repo_id, private=False)  # Utilisez private=True si vous voulez un dépôt privé\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chemin vers le fichier modèle à uploader\nfile_path = \"/kaggle/working/llama-3-8b-chat-doctor-merged.gguf\"\n\n# Pousser le fichier modèle sur Hugging Face\nupload_file(\n    path_or_fileobj=file_path,\n    path_in_repo=\"llama-3-8b-chat-doctor-merged.gguf\",  # Nom du fichier dans le dépôt\n    repo_id=repo_id,\n    commit_message=\"Ajout du modèle fusionné\",\n    token=hf_token\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:49:30.846093Z","iopub.execute_input":"2024-06-17T13:49:30.846449Z","iopub.status.idle":"2024-06-17T13:57:13.814804Z","shell.execute_reply.started":"2024-06-17T13:49:30.846415Z","shell.execute_reply":"2024-06-17T13:57:13.813776Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"llama-3-8b-chat-doctor-merged.gguf:   0%|          | 0.00/16.1G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806b68fe4e3b47c0b867b6bb8a9b9370"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/PierreMaxime/llama-3-8b-chat-doctor-merged/commit/ab11ee8fcb448356893f407b0f7421c594abca65', commit_message='Ajout du modèle fusionné', commit_description='', oid='ab11ee8fcb448356893f407b0f7421c594abca65', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}