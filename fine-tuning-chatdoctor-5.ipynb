{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":184010302,"sourceType":"kernelVersion"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# V. Application de la Quantification","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n%cd /kaggle/working/llama.cpp\n!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n!LLAMA_CUDA=1 conda run -n base make -j > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-17T14:51:22.017745Z","iopub.execute_input":"2024-06-17T14:51:22.018329Z","iopub.status.idle":"2024-06-17T14:55:48.619522Z","shell.execute_reply.started":"2024-06-17T14:51:22.018296Z","shell.execute_reply":"2024-06-17T14:55:48.618212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-06-17T14:56:35.841444Z","iopub.execute_input":"2024-06-17T14:56:35.842120Z","iopub.status.idle":"2024-06-17T14:56:36.844199Z","shell.execute_reply.started":"2024-06-17T14:56:35.842085Z","shell.execute_reply":"2024-06-17T14:56:36.843175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Appliquer le script de quantization\n!/kaggle/working/llama.cpp/llama-quantize /kaggle/input/fine-tuning-chatdoctor-4/llama-3-8b-chat-doctor-merged.gguf /kaggle/working/llama-3-8b-chat-doctor-merged-Q4_K_M.gguf Q4_K_M","metadata":{"execution":{"iopub.status.busy":"2024-06-17T14:56:57.915130Z","iopub.execute_input":"2024-06-17T14:56:57.916074Z","iopub.status.idle":"2024-06-17T15:05:20.517781Z","shell.execute_reply.started":"2024-06-17T14:56:57.916038Z","shell.execute_reply":"2024-06-17T15:05:20.516750Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-06-17T15:14:16.345437Z","iopub.execute_input":"2024-06-17T15:14:16.346414Z","iopub.status.idle":"2024-06-17T15:14:17.308217Z","shell.execute_reply.started":"2024-06-17T15:14:16.346378Z","shell.execute_reply":"2024-06-17T15:14:17.307081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pousser le modèle quantifier sur Hugging Face ppur ensuite pouvoir l'utiliser sur LM Studio. Si on souhaite l'utiliser sur JAN, il faut le télécharger sur l'ordinateur directement. \nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF \")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T15:15:59.924361Z","iopub.execute_input":"2024-06-17T15:15:59.925255Z","iopub.status.idle":"2024-06-17T15:16:00.268873Z","shell.execute_reply.started":"2024-06-17T15:15:59.925212Z","shell.execute_reply":"2024-06-17T15:16:00.267969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi, create_repo, upload_file\n\n# Identifiant du dépôt\nrepo_id = \"PierreMaxime/llama-3-8b-chat-doctor-merged-Q4_K_M\"\n\n# Initialisez l'API\napi = HfApi()\n\n# Créez le dépôt\napi.create_repo(repo_id=repo_id, private=False)  # Utilisez private=True si vous voulez un dépôt privé\n\n# Chemin vers le fichier modèle quantisé à uploader\nquantized_file_path = \"/kaggle/working/llama-3-8b-chat-doctor-merged-Q4_K_M.gguf\"\n\n# Pousser le fichier modèle quantisé sur Hugging Face\nupload_file(\n    path_or_fileobj=quantized_file_path,\n    path_in_repo=\"llama-3-8b-chat-doctor-merged-Q4_K_M.gguf\",  # Nom du fichier dans le dépôt\n    repo_id=repo_id,\n    commit_message=\"Ajout du modèle quantisé\",\n    token=hf_token\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T15:16:15.984256Z","iopub.execute_input":"2024-06-17T15:16:15.985077Z","iopub.status.idle":"2024-06-17T15:18:34.266055Z","shell.execute_reply.started":"2024-06-17T15:16:15.985040Z","shell.execute_reply":"2024-06-17T15:18:34.264969Z"},"trusted":true},"execution_count":null,"outputs":[]}]}