{"cells":[{"cell_type":"markdown","metadata":{},"source":["### 1. Importation + Connexion HF et Wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-26T08:59:17.563374Z","iopub.status.busy":"2024-06-26T08:59:17.562464Z","iopub.status.idle":"2024-06-26T09:01:27.367982Z","shell.execute_reply":"2024-06-26T09:01:27.366872Z","shell.execute_reply.started":"2024-06-26T08:59:17.563341Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["!pip install -U transformers\n","!pip install -U datasets\n","!pip install -U peft\n","!pip install -U bitsandbytes\n","\n","!pip install -U accelerate \n","!pip install -U trl\n","!pip install -U wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:01:27.370707Z","iopub.status.busy":"2024-06-26T09:01:27.370359Z","iopub.status.idle":"2024-06-26T09:01:49.005219Z","shell.execute_reply":"2024-06-26T09:01:49.003989Z","shell.execute_reply.started":"2024-06-26T09:01:27.370679Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","import pandas as pd\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","\n","from datasets import Dataset, load_dataset\n","from datasets import load_dataset, load_metric\n","from transformers import pipeline, set_seed\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:01:49.007933Z","iopub.status.busy":"2024-06-26T09:01:49.006730Z","iopub.status.idle":"2024-06-26T09:02:10.004397Z","shell.execute_reply":"2024-06-26T09:02:10.003199Z","shell.execute_reply.started":"2024-06-26T09:01:49.007893Z"},"trusted":true},"outputs":[],"source":["# Connexion à Hugging Face et à Wandb\n","from huggingface_hub import login\n","from kaggle_secrets import UserSecretsClient\n","import wandb\n","\n","!git config --global credential.helper store\n","user_secrets = UserSecretsClient()\n","\n","hf_token = user_secrets.get_secret(\"HF\")\n","\n","login(token=hf_token, add_to_git_credential=True)\n","\n","wb_token = user_secrets.get_secret(\"wandb\")\n","\n","wandb.login(key=wb_token)\n","\n","run = wandb.init(\n","    project='Fine-tune Summarization', \n","    job_type=\"training\", \n","    anonymous=\"allow\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Importation du Dataset "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:02:10.007309Z","iopub.status.busy":"2024-06-26T09:02:10.006938Z","iopub.status.idle":"2024-06-26T09:02:10.014304Z","shell.execute_reply":"2024-06-26T09:02:10.013174Z","shell.execute_reply.started":"2024-06-26T09:02:10.007281Z"},"trusted":true},"outputs":[],"source":["# Introduire le modèle de base + Dataset de training + nom du nouveau modèle\n","base_model = \"/kaggle/input/llama-3/transformers/8b-hf/1\"\n","dataset_name = \"cnn_dailymail\"\n","new_model = \"llama-3-8b-summarize\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:02:10.015732Z","iopub.status.busy":"2024-06-26T09:02:10.015449Z","iopub.status.idle":"2024-06-26T09:02:26.177304Z","shell.execute_reply":"2024-06-26T09:02:26.176084Z","shell.execute_reply.started":"2024-06-26T09:02:10.015708Z"},"trusted":true},"outputs":[],"source":["# Importation du dataset\n","dataset = load_dataset(dataset_name, \"3.0.0\")"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Preprocessing "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:02:26.179122Z","iopub.status.busy":"2024-06-26T09:02:26.178698Z","iopub.status.idle":"2024-06-26T09:02:26.187557Z","shell.execute_reply":"2024-06-26T09:02:26.186516Z","shell.execute_reply.started":"2024-06-26T09:02:26.179084Z"},"trusted":true},"outputs":[],"source":["def format_instruction(dialogue: str, summary: str):\n","    return f\"\"\"### Instruction:\n","Summarize the following conversation.\n","\n","### Input:\n","{dialogue.strip()}\n","\n","### Summary:\n","{summary}\n","\"\"\".strip()\n","\n","def generate_instruction_dataset(data_point):\n","    return {\n","        \"article\": data_point[\"article\"],\n","        \"highlights\": data_point[\"highlights\"],\n","        \"text\": format_instruction(data_point[\"article\"],data_point[\"highlights\"])\n","    }\n","\n","def process_dataset(data: dataset):\n","    return (\n","        data.map(generate_instruction_dataset).remove_columns(['id'])\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:02:26.189662Z","iopub.status.busy":"2024-06-26T09:02:26.189019Z","iopub.status.idle":"2024-06-26T09:03:04.076618Z","shell.execute_reply":"2024-06-26T09:03:04.075495Z","shell.execute_reply.started":"2024-06-26T09:02:26.189633Z"},"trusted":true},"outputs":[],"source":["# Preprocessing des données \n","dataset[\"train\"] = process_dataset(dataset[\"train\"])\n","dataset[\"test\"] = process_dataset(dataset[\"validation\"])\n","dataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n","\n","# Splits des données \n","train_data = dataset['train'].select([i for i in range(1000)]) # on peut ajouter un .shuffle(seed=42)\n","\n","test_data = dataset['test'].select([i for i in range(100)])\n","validation_data = dataset['validation'].select([i for i in range(100)])\n","\n","train_data,test_data,validation_data"]},{"cell_type":"markdown","metadata":{},"source":["### 4. Modification de Llama3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:03:04.078771Z","iopub.status.busy":"2024-06-26T09:03:04.078030Z","iopub.status.idle":"2024-06-26T09:03:04.084737Z","shell.execute_reply":"2024-06-26T09:03:04.083577Z","shell.execute_reply.started":"2024-06-26T09:03:04.078730Z"},"trusted":true},"outputs":[],"source":["torch_dtype = torch.float16 # Définition du type de données utilisé par PyTorch pour les calculs tensoriels\n","attn_implementation = \"eager\" # Opération excécutée immédiatement (top pour le jupyter notebook) ≠ \"graph\" (mieux pour des gros modèles)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:03:04.086682Z","iopub.status.busy":"2024-06-26T09:03:04.086366Z","iopub.status.idle":"2024-06-26T09:05:08.159709Z","shell.execute_reply":"2024-06-26T09:05:08.158497Z","shell.execute_reply.started":"2024-06-26T09:03:04.086654Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# Configuration de QLoRA (méthode de quantification : Quantized Low Rank Adapter), obligatoire car on a une contrainte de mémoire\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch_dtype,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","# Chargement du modèle\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    attn_implementation=attn_implementation\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:05:08.165460Z","iopub.status.busy":"2024-06-26T09:05:08.164993Z","iopub.status.idle":"2024-06-26T09:05:08.795405Z","shell.execute_reply":"2024-06-26T09:05:08.794233Z","shell.execute_reply.started":"2024-06-26T09:05:08.165416Z"},"trusted":true},"outputs":[],"source":["# Chargement du tokenizer (ChatML template qui distingue l'user de l'assistant)\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# Si on a un dataset avec un user et un assistant : model, tokenizer = setup_chat_format(model, tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["### 5. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:05:08.797416Z","iopub.status.busy":"2024-06-26T09:05:08.796917Z","iopub.status.idle":"2024-06-26T09:05:09.866442Z","shell.execute_reply":"2024-06-26T09:05:09.865081Z","shell.execute_reply.started":"2024-06-26T09:05:08.797370Z"},"trusted":true},"outputs":[],"source":["from peft import prepare_model_for_kbit_training\n","\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:05:09.868336Z","iopub.status.busy":"2024-06-26T09:05:09.867911Z","iopub.status.idle":"2024-06-26T09:05:10.313875Z","shell.execute_reply":"2024-06-26T09:05:10.312397Z","shell.execute_reply.started":"2024-06-26T09:05:09.868305Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","\n","# Configuration de LoRA, paramètres pour améliorer le temps d'entrainement \n","peft_config = LoraConfig(\n","    r=16,\n","    lora_alpha=64,# ou 32 à voir pour les bons paramètres à utiliser\n","    lora_dropout=0.1, # ou 0.1 mais tj checker pour les paramètres \n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",")\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:05:10.319679Z","iopub.status.busy":"2024-06-26T09:05:10.315306Z","iopub.status.idle":"2024-06-26T09:05:10.359617Z","shell.execute_reply":"2024-06-26T09:05:10.358109Z","shell.execute_reply.started":"2024-06-26T09:05:10.319637Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n","\n","training_arguments = TrainingArguments(\n","    output_dir=new_model,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=2,\n","    optim=\"adamw_torch\", # ou optim=\"paged_adamw_32bit\",\n","    logging_steps=1,\n","    learning_rate=1e-4,\n","    fp16=False,\n","    max_grad_norm=0.3,\n","    num_train_epochs=2,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=0.2,\n","    warmup_ratio=0.05,\n","    save_strategy=\"epoch\",\n","    group_by_length=True,\n","    report_to=\"wandb\",\n","    save_safetensors=True,\n","    lr_scheduler_type=\"cosine\",\n",")\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:05:10.361565Z","iopub.status.busy":"2024-06-26T09:05:10.361062Z","iopub.status.idle":"2024-06-26T09:05:12.695281Z","shell.execute_reply":"2024-06-26T09:05:12.693880Z","shell.execute_reply.started":"2024-06-26T09:05:10.361524Z"},"trusted":true},"outputs":[],"source":["from trl import SFTTrainer\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_data,\n","    eval_dataset=validation_data,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=1024,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T09:05:12.697819Z","iopub.status.busy":"2024-06-26T09:05:12.697288Z","iopub.status.idle":"2024-06-26T09:47:13.432397Z","shell.execute_reply":"2024-06-26T09:47:13.429983Z","shell.execute_reply.started":"2024-06-26T09:05:12.697768Z"},"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:47:13.433701Z","iopub.status.idle":"2024-06-26T09:47:13.434257Z","shell.execute_reply":"2024-06-26T09:47:13.433979Z","shell.execute_reply.started":"2024-06-26T09:47:13.433957Z"},"trusted":true},"outputs":[],"source":["wandb.finish()\n","model.config.use_cache = True"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Inférence sur le modèle non merge avec llama3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:47:13.436962Z","iopub.status.idle":"2024-06-26T09:47:13.437651Z","shell.execute_reply":"2024-06-26T09:47:13.437384Z","shell.execute_reply.started":"2024-06-26T09:47:13.437355Z"},"trusted":true},"outputs":[],"source":["# Remettre la mémoire cache \n","from transformers import TextStreamer\n","model.config.use_cache = True\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:47:13.438765Z","iopub.status.idle":"2024-06-26T09:47:13.439179Z","shell.execute_reply":"2024-06-26T09:47:13.438969Z","shell.execute_reply.started":"2024-06-26T09:47:13.438953Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","os.environ[\"TOKEN\"] = \"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\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:47:13.440799Z","iopub.status.idle":"2024-06-26T09:47:13.441241Z","shell.execute_reply":"2024-06-26T09:47:13.441022Z","shell.execute_reply.started":"2024-06-26T09:47:13.441005Z"},"trusted":true},"outputs":[],"source":["index = 51\n","\n","dialogue = train_data['article'][index][:10000]\n","summary = train_data['highlights'][index]\n","\n","prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","### Input:\n","{dialogue}\n","\n","### Summary:\n","\"\"\"\n","\n","input_ids = tokenizer(prompt, return_tensors='pt',truncation=True).input_ids.cuda()\n","outputs = trained_model.generate(input_ids=input_ids, max_new_tokens=200, )\n","output= tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'TRAINED MODEL GENERATED TEXT :\\n{output}')"]},{"cell_type":"markdown","metadata":{},"source":["### 7. Enregistrement du modèle finetune"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:47:13.443004Z","iopub.status.idle":"2024-06-26T09:47:13.443445Z","shell.execute_reply":"2024-06-26T09:47:13.443268Z","shell.execute_reply.started":"2024-06-26T09:47:13.443250Z"},"trusted":true},"outputs":[],"source":["peft_model_path=\"./peft-dialogue-summary\"\n","\n","trainer.model.save_pretrained(peft_model_path)\n","tokenizer.save_pretrained(peft_model_path)"]},{"cell_type":"markdown","metadata":{},"source":["### Zero Shot"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:47:13.444601Z","iopub.status.idle":"2024-06-26T09:47:13.444958Z","shell.execute_reply":"2024-06-26T09:47:13.444796Z","shell.execute_reply.started":"2024-06-26T09:47:13.444780Z"},"trusted":true},"outputs":[],"source":["from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer\n","\n","peft_model_dir = \"peft-dialogue-summary\"\n","\n","# load base LLM model and tokenizer\n","trained_model = AutoPeftModelForCausalLM.from_pretrained(\n","    peft_model_dir,\n","    low_cpu_mem_usage=True,\n","    torch_dtype=torch.float16,\n","    load_in_4bit=True,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(peft_model_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:47:13.446389Z","iopub.status.idle":"2024-06-26T09:47:13.446760Z","shell.execute_reply":"2024-06-26T09:47:13.446595Z","shell.execute_reply.started":"2024-06-26T09:47:13.446579Z"},"trusted":true},"outputs":[],"source":["index = 2\n","\n","dialogue = test_data['article'][index]\n","summary = test_data['highlights'][index]\n","\n","prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","### Input:\n","{dialogue}\n","\n","### Summary:\n","\"\"\"\n","\n","inputs = tokenizer(prompt, return_tensors='pt')\n","output = tokenizer.decode(\n","    model.generate(\n","        inputs[\"input_ids\"],\n","        max_new_tokens=100,\n","    )[0],\n","    skip_special_tokens=True\n",")\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"isSourceIdPinned":true,"modelInstanceId":28079,"sourceId":33547,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python (myenv)","language":"python","name":"myenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
